\chapter{人間のチーム行動解析}
%章番号が不要な場合は『\chapter*{概要}』とする

\section{自己組織化マップ（SOM）}
自己組織化とは，局所的な相互作用から地域的に順序づけられた構造が導かれることであり，複数の要素から構成されるシステムが他からの制御を得ることなく，時間とともに何らかの意味で自発的に秩序化する過程である．神経回路の構造は，脳内においてこの自己組織化の代表的な例として挙げられる．1956年にSperry[s]によってはじめられた視覚神経通路の発達における位相的順序を保持した状態での網膜位相マップの構築を目的としたモデル化が，自己組織化の位相形成原理についてのモデル化の先駆けとなり，Von der Malsburg[?]が特徴選択における皮質細胞の局所的な順序づけを目的としたシナプス学習を含む自己組織化過程を導入するなど，創刊がある神経活動によって駆動される位相的なマップの形成に関係した自己組織化の要素が神経活動による位相マップの初期の形成には存在するという結論が主流となった．その後，1976年にWillshaw とvon der Malsburg[?]によって，本質的には同様の幾何学的関係がある離散的な格子間での類似関係を検出する自己組織化処理モデルが，自己組織化マップ（Self-Organizing Map : SOM）として提案され，SOMは，教師なし学習であり，生命の神経回路による情報処理に近い競合学習として知られるようになった．\\
Kohonen は前述したモデルからさらに，類似関係があるものではない，かつ幾何学条性格である必要もない離散的格子上に連続した入力君間からマップ化することを目的として学習アルゴリズムを変更した[?].より強度な自己組織化効果を得るために，近傍関係を学習に導入するため荷重更新規則を修正し，これらの近傍関係によって広がった範囲が時間経過にともない縮まる要素も追加した．これにより，SOMのアルゴリズムとして，Kohonenモデルが一般的となり，本論文中にあいてもKohonenモデルを取り扱う．\\
この構造により，多次元入力ベクトルの低次元化が出力そうにて可能であり，特徴が近似している入力ベクトルほど，近くに配置される．SOMの応用例としては，複雑なデータの二次元表示への可視化があり，クラスタリング技術と同様に，抽象化の創造が可能である．優れた補間性能を保つために，学習データの一般化が可能であり，未知な入力データに対しても妥当性の高い出力が期待される．
\subsection{競合学習}
競合学習は階層型ニューラルネットワークにおいて重要な懸念である．同一の入力に対し，各々のニューロンが活性度を競い合う側方相互作用が利用される．具体的に競合学習としてのSOMは，入力層と出力層からなり，入力層(入力空間V)の各入力サンプル（入力ベクトルv(t)）は全ての出力層ニューロンと結合し，結合には結合荷重wiが付加されている．ここでtは時間座標であり，N個の出力層ニューロンは，i=1,2,・・・,Nとラベル付けされている．(Fig参照)そして，近くのニューロンとの結合は興奮性（正の結合荷重）があり，より遠方のニューロンとの結合には抑制性（負の結合荷重）がある状態で出力層のニューロンは活性化するために競合する．
\subsection{アルゴリズム}
KohonenのSOMアルゴリズムは，大きく競合段階と協調段階の二つの改装で構成される．最初の段階ではまず，入力に対して，最整合ニューロン（Best Matching Unit : BMU）の選択すなわち勝者を選択する．次に，その最整合ニューロンおよびその直近の格子の荷重変更が行われる．以下に，これら二つの段階について述べるが，本研究では，新しい入力ベクトルの投入のたびに，荷重更新を行う増分学習（オンライン学習）ではなく，全入力ベクトルの協業段階が行われた後に，生成できる全入力が考慮された荷重更を行う，バッチ型SOM[?]を用いる．これにより，入力ベクトルの投入順番による出力への影響を考慮する必要がなくなる．よって，以降は本研究で用いたバッチ型SOMについて述べているため，バッチ型SOMへの変更に伴い，入力空間$V$を$ \mu = 1, \cdots，M$の$M$個のサンプルからなる固定の学習ベクトル集合$\{v^\mu\}$として定義する．
\subsubsection{競合段階}
競合段階では，各入力ベクトル$v^\mu$に対する最整合ニューロンについて式(\ref{1})を用いて選択する．BMUの決定方法としては，内積距離規則やユークリッド距離規則が考えられる．内積距離規則は生物学的モデルに類似しているため，多くその方面で用いられているが，今回は生物学的なモデルとしてSOMを扱う必要がないため，ユークリッド距離規則によるBMU決定を採用する．ユークリッド距離を用いる場合，結合荷重改め，ベクトル量子化の観点から参照ベクトルと呼ぶこととする．ここでは，勝者$j*$とラベル付けする．また，$\|.\|$はユークリッド距離を表している．
\begin{equation}
\label{1}
  i^* = argmin||w_i - v^\mu||
\end{equation}

\subsubsection{協調段階}
協調段階では，ニューロンが発火（BMUが決定されること）することにより，その周囲のニューロンも協調的に発火しやすくなるように荷重更新に影響を与える段階である．各丹生論の参照ベクトル$w_i$を式(\ref{2})により更新する．ここで，$t$は学習回数を示す変数であり，総学習回数を$t_{max}$とする．$\eta$は学習係数である．

\begin{equation}
\label{2}
w_i(t+1) = w_i(t)+\frac{\eta}{M}・\Delta w_i
\end{equation}

ただし，参照ベクトルの増減値$\Delta w_i$は，次式（\ref{3}）で示される．

\begin{equation}
\label{3}
\Delta w_i(t +1) = \Delta w_i(t)+\Lambda(i,i^*,\sigma_\Lambda)・（v^\mu-w_i）
\end{equation}

ここで，$\Lambda$はニューロン$i$と $i^*$の格子座標$r$と$r^*$での尺度調整を行う近傍関数である．勝者の位置に関して左右対称であり，勝者からの格子距離が増加するにしたがって単調に減少する関数でなければならない．今回は，もっとも一般的なガウス型近傍関数(式\ref{4}参照)を用いた．（図[?]参照）$\sigma_\Lambda$は，その範囲(標準偏差)である．

\begin{equation}
\label{4}
\Lambda(i, i^*) = \exp(-\frac{{\|r_i-r_{i^*}\|}^2}{{2\sigma_\Lambda}^2})
\end{equation}

先に述べた，近傍関数によって広がった範囲が時間経過にともない縮まる要素として，標準偏差，式(\ref{5})
によって学習回数とともに小さな値をとる．(図?参照)

\begin{equation}
\label{5}
\sigma_\Lambda={\sigma_{\Lambda_0}}\exp(-2{\sigma_{\Lambda_0}}\frac{t}{t_{max}})
\end{equation}





